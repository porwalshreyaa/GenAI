# Vector Embeddings

## What are Vector Embeddings?

Vector embedding are 'digital fingerprints' or 'numerical representations' of words or other pieces of data. Each object is transformed into a list of numbers called a vector. These vectors captures properties of the object in a more manageable and understandable form for machine learning models.

- numerical representations (lists of numbers) = vectors
- object = data points that express different types of data

### What are Vectors?
A vector is a one dimensional array of numbers containing multiple scalars of the same type of data.
- Vectors represents properties, features in a more machine understandable way.

> Vector embeddings are numerical representations of data points that express different types of data, including nonmathematical data such as words or images, as an array of numbers that machine learning (ML) models can process.

### Uses
- Compare Similarities
- Clustering (group related data)
- Perform Arithmetic operations
- Feed machine understandable data

### types
1. Word Embedding: a numeric representation (vector) of a word in a lower-dimensional space. Capture semantic information and also their contextual relationship to other words (syntactic information)
2. Sentence Embedding
3. Image Embedding
4. Multimodel Embedding



sources:
- [geeksforgeeks](https://www.geeksforgeeks.org/nlp/what-are-vector-embeddings/)
- [IBM](https://www.ibm.com/think/topics/vector-embedding#1003835712)